{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Attribution\n",
    "Dialogue attribution is an important component of comprehending films. While we currently have a ground-truth transcription of all spoken dialogue (in the subtitles), we need to be able to tie these sentences to the speaker of that line.\n",
    "\n",
    "This isn't the first time we've attempted dialogue attribution. In the directory `/depreciated_modules/dialogue_attribution`, we attempted automated speaker diarization of voices, assigning sentences to one of two voice encodings. A conversation would be divided into sentences by speaker M and speaker N, but these would sometimes be entirely flipped, attributing almost all lines to the incorrect speaker.\n",
    "\n",
    "Another attempt was made with the function `get_implied_speaker()`. This would calculate the frames onscreen during the entire duration of the subtitle, and try and count the onscreen faces which have their mouth open. This would return the face encoding of the character onscreen, but also wasn't reliably accurate.\n",
    "\n",
    "After continued development in visual, audio, and subtitle, we can make another attempt to solve dialogue attribution. It will be a combination of the two approaches developed above: diarizing the conversation audio, and then looking for onscreen faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../subtitle_features')\n",
    "sys.path.append('../audio_features')\n",
    "sys.path.append('../data_serialization')\n",
    "from serialization_preprocessing_io import *\n",
    "from time_reference_io import *\n",
    "from film_details_io import *\n",
    "from scene_identification_io import *\n",
    "from scene_details_io import *\n",
    "from character_identification_io import *\n",
    "from character_details_io import *\n",
    "from subtitle_dataframes_io import *\n",
    "import pyAudioAnalysis.audioSegmentation\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Scene\n",
    "We'll work with the first scene identified in *Lost in Translation* (2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'lost_in_translation_2003'\n",
    "srt_df, subtitle_df, sentence_df, vision_df, face_df = read_pickle(film)\n",
    "scene_dictionaries = generate_scenes(vision_df, face_df, substantial_minimum=4, anchor_search=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene_id': 1,\n",
       " 'first_frame': 1929,\n",
       " 'last_frame': 2082,\n",
       " 'scene_duration': 154,\n",
       " 'left_anchor_shot_cluster': 207,\n",
       " 'left_anchor_face_cluster': 6.0,\n",
       " 'matching_left_face_clusters': [],\n",
       " 'right_anchor_shot_cluster': 66,\n",
       " 'right_anchor_face_cluster': 17.0,\n",
       " 'matching_right_face_clusters': [18.0, 12.0],\n",
       " 'cutaway_shot_clusters': [7, 29, 142]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_dictionaries[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
