{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Boundary Paritioning\n",
    "Movies are comprised of self-contained units called scenes. Scenes have a beginning and end, usually comprised of a single conversation. They most often take place in one location with a fixed number of characters. By identifying scenes in a movie, we can then begin to analyze them individually, most notably by treating a scenes's dialogue as a freestanding, indepdent conversation.\n",
    "\n",
    "To start, we'll just be identifying two-character dialogue scenes. These are the most basic building-blocks of films: just two characters speaking together with no distractions, purely advancing the plot with their dialogue. In modern filmmaking, these scenes are usually shot in a specific manner. We can take advantage of this by looking for specific patterns of shots, to identify a few two-character dialogue scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data_serialization')\n",
    "from serialization_preprocessing_io import *\n",
    "from time_reference_io import *\n",
    "from scene_identification_io import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved pickle objects of various dataframes. We'll load into memory the five dataframes, but we're most interested in the two which deal with onscreen images. The each have one row per frame (screencap), with one frame per second â€” so each row represents one second of onscreen action.\n",
    "- vision_df: contains general computer vision information on each frame, including clusterings of similar frames into \"shots\"\n",
    "- face_df: contains information related to faces found, including their vectorized encodings, and clusters of these encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'plus_one_2019'\n",
    "srt_df, subtitle_df, sentence_df, vision_df, face_df = read_pickle(film)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Scenes\n",
    "In modern film, two-character dialogue scenes follow a very distinct pattern. Character A speaks, then Character B, then back to A, then to B, etc. We cut back and forth between the two characters.\n",
    "\n",
    "\n",
    "### Anchor Shots: The A/B/A/B Pattern\n",
    "We look for these two Anchor shots, which are the shots of the two characters and form the A/B/A/B pattern. We'll be looking through every frame in the film, and trying to find these ABAB patterns.\n",
    "\n",
    "The key to this lies in two columns in vision_df:\n",
    "- shot_cluster:  represents clusters of similar frames, or shots. Think of a four-second shot of a character speaking. This would be represented as four rows with a common shot_cluster\n",
    "- shot_id: sequential numbering of each shot (regardless of uniqueness). Every time a shot changes (and even if we've seen this shot before), the shot_id is incremented by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blank</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>brightness</th>\n",
       "      <th>contrast</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>shot_cluster</th>\n",
       "      <th>shot_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>65</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>100</td>\n",
       "      <td>61</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>114</td>\n",
       "      <td>71</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>92</td>\n",
       "      <td>68</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>102</td>\n",
       "      <td>186</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>92</td>\n",
       "      <td>69</td>\n",
       "      <td>81</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>186</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>102</td>\n",
       "      <td>186</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>96</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>107</td>\n",
       "      <td>186</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>None</td>\n",
       "      <td>2.39</td>\n",
       "      <td>73</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>69</td>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blank  aspect_ratio  brightness  contrast  blue  green  red  \\\n",
       "frame                                                               \n",
       "203    None          2.39          39        39    36     36   45   \n",
       "204    None          2.39          31        28    25     27   42   \n",
       "205    None          2.39          35        36    33     32   44   \n",
       "206    None          2.39          29        30    25     25   39   \n",
       "207    None          2.39          65        51    57     63   73   \n",
       "208    None          2.39         100        61    89     95  114   \n",
       "209    None          2.39          92        68    82     88  102   \n",
       "210    None          2.39          92        69    81     88  103   \n",
       "211    None          2.39          91        67    82     88  102   \n",
       "212    None          2.39          96        68    86     92  107   \n",
       "213    None          2.39          73        60    64     69   84   \n",
       "\n",
       "       shot_cluster  shot_id  \n",
       "frame                         \n",
       "203               2       52  \n",
       "204               2       52  \n",
       "205               2       52  \n",
       "206               2       52  \n",
       "207               2       52  \n",
       "208              71       53  \n",
       "209             186       54  \n",
       "210             186       54  \n",
       "211             186       54  \n",
       "212             186       54  \n",
       "213               5       55  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_df[202:213]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will generate two lists each time an ABAB pattern is found:\n",
    "- alternating_pairs: the two shot_clusters\n",
    "- pair_shot_ids: the beginning and ending shot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "shot_id_list = vision_df.shot_id.tolist()\n",
    "shot_clusters = vision_df.shot_cluster.tolist()\n",
    "frame_choice = range(1, (len(vision_df) + 1))\n",
    "\n",
    "# to check for an A/B/A/B pattern, we must store the previous three clusters in memory\n",
    "prev_clust_1 = 1001\n",
    "prev_clust_2 = 1002\n",
    "prev_clust_3 = 1003\n",
    "prev_shot_id = -1\n",
    "alternate_a_list = []\n",
    "alternate_b_list = []\n",
    "pair_shot_ids = []\n",
    "pair_found = 0\n",
    "\n",
    "# zip our various lists into a usable data structure\n",
    "for frame_file, cluster, shot_id in zip(frame_choice, shot_clusters, shot_id_list):\n",
    "\n",
    "\n",
    "    # we use prev_shot_id to identify when there's a new shot (when the cluster value changes)\n",
    "    # when iterating through each frame, look for an A/B/A/B pattern, and save the clusters of any patterns\n",
    "    if shot_id != prev_shot_id:\n",
    "        if cluster == prev_clust_2 and prev_clust_1 == prev_clust_3:\n",
    "            if pair_found == 0:\n",
    "                alternate_a_list.append(min(cluster, prev_clust_1)) # min and max are used to avoid duplicates of (1, 2), (2, 1)\n",
    "                alternate_b_list.append(max(cluster, prev_clust_1))\n",
    "                beginning_shot = shot_id - 3\n",
    "            pair_found = 1\n",
    "        else:\n",
    "            if pair_found == 1:\n",
    "                ending_shot = shot_id - 1\n",
    "                pair_shot_ids.append([beginning_shot, ending_shot])\n",
    "            pair_found = 0\n",
    "        \n",
    "        # every time there's a new shot, we update the cluster memory\n",
    "        prev_shot_id = shot_id\n",
    "        prev_clust_3 = prev_clust_2\n",
    "        prev_clust_2 = prev_clust_1\n",
    "        prev_clust_1 = cluster\n",
    "        \n",
    "    # the below print can be used for troubleshooting and visualizing the memory state at each frame\n",
    "    # print(frame_file, '\\t', mcu_flag, '\\t', cluster,'\\t', shot_id, '\\t', prev_shot_id, '\\t', prev_clust_1, '\\t', prev_clust_2, '\\t', prev_clust_3, '\\tend')\n",
    "\n",
    "# save non-unique alternating pairs, because these must line up with pair_shot_ids\n",
    "alternating_pairs = []\n",
    "\n",
    "for a, b, in zip(alternate_a_list, alternate_b_list):\n",
    "    alternating_pairs.append([int(a), int(b)])\n",
    "\n",
    "print(len(alternating_pairs))\n",
    "print(len(pair_shot_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65, 334]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternating_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 36]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_shot_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Substantial Pairs\n",
    "We've found 108 instances of scenes where there's an ABAB pattern, but we may want to filter this down. Below, we define a threshold of 6 alternating shots. We check if the shot_id's differ by six or more shots. In other words, we're looking for a series of shots which form a ABABAB pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "substantial_pair_shot_ids = []\n",
    "substantial_anchor_shot_clusters = []\n",
    "threshold = 6\n",
    "\n",
    "for anchor_pair, shot_id_pair in zip(alternating_pairs, pair_shot_ids):\n",
    "    if shot_id_pair[1] - shot_id_pair[0] > threshold:\n",
    "        substantial_pair_shot_ids.append(shot_id_pair)\n",
    "        substantial_anchor_shot_clusters.append(anchor_pair)\n",
    "print(len(substantial_pair_shot_ids))\n",
    "print(len(substantial_anchor_shot_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Faces\n",
    "Next, we check each of the 22 possible dialogue scenes to make sure they actuallly contain characters speaking. We do this by checking both anchor shots, and making sure there's actually faces in them. First, we'll need to define functions which return the majority face clusters (and other face clusters) in each anchor. Remember, for each pair of anchor shots, one of them has a character on the left side of the screen, and the other has a different character on the right side of the screen.\n",
    "\n",
    "We only want to keep anchor shot pairs that find faces on the left, and other faces on the right. We only keep anchor shot pairs where one anchor has faces on the left in more than 50% of its frames, and the other anchor has faces on the right in more than 50% of its frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_face_clusters(alternation_face_df):\n",
    "    \"\"\"\n",
    "    returns the primary face cluster for the left character in a scene, and a list of additional matching face clusters\n",
    "    primary face cluster is the most prevalent, and matching clusters are the rest\n",
    "    \"\"\"\n",
    "    matching_left_clusters = []\n",
    "\n",
    "    left_value_counts = alternation_face_df[(alternation_face_df['p_center_alignment'] == 'left') & (alternation_face_df['faces_found'] == 1)].p_face_cluster.value_counts(normalize=True)\n",
    "\n",
    "    if len(alternation_face_df[(alternation_face_df['p_center_alignment'] == 'left') & (alternation_face_df['faces_found'] == 1)]) > 2:\n",
    "        if left_value_counts.values[0] >= .5:\n",
    "            left_anchor_face_cluster = left_value_counts.index.values[0]\n",
    "            left_anchor_face_encoding = np.average(alternation_face_df.loc[(alternation_face_df['p_center_alignment'] == 'left') & (alternation_face_df['p_face_cluster'] == left_anchor_face_cluster) & (alternation_face_df['faces_found'] == 1)].face_encodings.tolist(), axis=0)[0]\n",
    "            for candidate in left_value_counts.index.values[1:]:\n",
    "                left_cluster_candidate = np.average(alternation_face_df.loc[(alternation_face_df['p_center_alignment'] == 'left') & (alternation_face_df['p_face_cluster'] == candidate) & (alternation_face_df['faces_found'] == 1)].face_encodings.tolist(), axis=0)[0]\n",
    "                if face_recognition.compare_faces([left_anchor_face_encoding], left_cluster_candidate)[0] == True:\n",
    "                    matching_left_clusters.append(candidate)\n",
    "            return left_anchor_face_cluster, matching_left_clusters\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_face_clusters(alternation_face_df):\n",
    "    \"\"\"\n",
    "    returns the primary face cluster for the right character in a scene, and a list of additional matching face clusters\n",
    "    primary face cluster is the most prevalent, and matching clusters are the rest\n",
    "    \"\"\"\n",
    "    matching_right_clusters = []\n",
    "\n",
    "    right_value_counts = alternation_face_df[(alternation_face_df['p_center_alignment'] == 'right') & (\n",
    "                alternation_face_df['faces_found'] == 1)].p_face_cluster.value_counts(normalize=True)\n",
    "\n",
    "    if len(alternation_face_df[\n",
    "               (alternation_face_df['p_center_alignment'] == 'right') & (alternation_face_df['faces_found'] == 1)]) > 2:\n",
    "        if right_value_counts.values[0] >= .5:\n",
    "            right_anchor_face_cluster = right_value_counts.index.values[0]\n",
    "            right_anchor_face_encoding = np.average(alternation_face_df.loc[\n",
    "                                                        (alternation_face_df['p_center_alignment'] == 'right') & (\n",
    "                                                                    alternation_face_df[\n",
    "                                                                        'p_face_cluster'] == right_anchor_face_cluster) & (\n",
    "                                                                    alternation_face_df[\n",
    "                                                                        'faces_found'] == 1)].face_encodings.tolist(),\n",
    "                                                    axis=0)[0]\n",
    "            for candidate in right_value_counts.index.values[1:]:\n",
    "                right_cluster_candidate = np.average(alternation_face_df.loc[\n",
    "                                                         (alternation_face_df['p_center_alignment'] == 'right') & (\n",
    "                                                                     alternation_face_df[\n",
    "                                                                         'p_face_cluster'] == candidate) & (\n",
    "                                                                     alternation_face_df[\n",
    "                                                                         'faces_found'] == 1)].face_encodings.tolist(),\n",
    "                                                     axis=0)[0]\n",
    "                if face_recognition.compare_faces([right_anchor_face_encoding], right_cluster_candidate)[0] == True:\n",
    "                    matching_right_clusters.append(candidate)\n",
    "            return right_anchor_face_cluster, matching_right_clusters\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "alternating_scene_frame_pairs = []\n",
    "alternating_scene_anchor_pairs = []\n",
    "\n",
    "for pair, anchors in zip(substantial_pair_shot_ids, substantial_anchor_shot_clusters):\n",
    "    first_frame = vision_df[vision_df['shot_id'].isin([pair[0], pair[1]])][:1].index[0]\n",
    "    last_frame = vision_df[vision_df['shot_id'].isin([pair[0], pair[1]])][-1:].index[0]\n",
    "    alternation_face_df = face_df.copy()[first_frame - 1:last_frame]\n",
    "    left_right_percentage = len(\n",
    "        alternation_face_df[alternation_face_df['p_center_alignment'].isin(['left', 'right'])]) / len(\n",
    "        alternation_face_df) * 100\n",
    "    prim_face_percentage = len(alternation_face_df[alternation_face_df['prim_char_flag'] == 1]) / len(\n",
    "        alternation_face_df) * 100\n",
    "    left_anchor_face_cluster, matching_left_clusters = left_face_clusters(alternation_face_df)\n",
    "    right_anchor_face_cluster, matching_right_clusters = right_face_clusters(alternation_face_df)\n",
    "    if left_anchor_face_cluster and right_anchor_face_cluster:\n",
    "        if prim_face_percentage >= .8:\n",
    "            alternating_scene_frame_pairs.append([first_frame, last_frame])\n",
    "            alternating_scene_anchor_pairs.append(anchors)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print(len(alternating_scene_frame_pairs))\n",
    "print(len(alternating_scene_anchor_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding the Scene\n",
    "Next, we'll take a single scene and expand it, beyond just the anchors (the two individual shots of Characters A and B). This will help find cutaways, shots that are still part of the scene, but aren't the anchor shots. Cutaways may include shots like a closeup of an object or a POV shot of where a character is looking.\n",
    "\n",
    "We approach this by looking for the anchor shots, which fall outside the scope of the ABAB pattern, but nearby. For example, we might have a scene with ACBABAB, where C is a cutaway. We look before the first, and after the last shot in the ABAB pattern for the anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[492, 527]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternating_scene_frame_pair = alternating_scene_frame_pairs[0]\n",
    "alternating_scene_frame_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[483, 527]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_search_threshold = 6\n",
    "\n",
    "anchor_shot_cluster_pair = list(vision_df[alternating_scene_frame_pair[0] - 1:alternating_scene_frame_pair[1]].shot_cluster.unique())\n",
    "anchor_shot_id_pair = [vision_df[alternating_scene_frame_pair[0] - 1:alternating_scene_frame_pair[1]].shot_id.min(),\n",
    "                       vision_df[alternating_scene_frame_pair[0] - 1:alternating_scene_frame_pair[1]].shot_id.max()]\n",
    "first_anchor_frame = vision_df[(vision_df['shot_id'] > anchor_shot_id_pair[0] - anchor_search_threshold) & (vision_df['shot_id'] < anchor_shot_id_pair[1] + anchor_search_threshold) & (vision_df['shot_cluster'].isin(anchor_shot_cluster_pair))].index.min()\n",
    "last_anchor_frame = vision_df[\n",
    "    (vision_df['shot_id'] > anchor_shot_id_pair[0] - anchor_search_threshold) & (vision_df['shot_id'] < anchor_shot_id_pair[1] + anchor_search_threshold) & (\n",
    "        vision_df['shot_cluster'].isin(anchor_shot_cluster_pair))].index.max()\n",
    "cutaways = vision_df[first_anchor_frame - 1:last_anchor_frame].shot_cluster.unique()\n",
    "cutaways = cutaways[cutaways != anchor_shot_cluster_pair[0]] # remove the Speaker A and Speaker B clusters from this list\n",
    "cutaways = cutaways[cutaways != anchor_shot_cluster_pair[1]]\n",
    "\n",
    "scene_start_frame = first_anchor_frame\n",
    "min_flag = 0\n",
    "\n",
    "while min_flag == 0:\n",
    "    try:\n",
    "        if vision_df.loc[scene_start_frame - 1].shot_cluster in cutaways:\n",
    "            scene_start_frame -= 1\n",
    "        else:\n",
    "            min_flag = 1\n",
    "    except TypeError:  # error if hitting the beginning of the frame list\n",
    "        min_flag = 1\n",
    "\n",
    "scene_end_frame = last_anchor_frame\n",
    "max_flag = 0\n",
    "while max_flag == 0:\n",
    "    try:\n",
    "        if vision_df.loc[scene_start_frame - 1].shot_cluster in cutaways:\n",
    "            scene_end_frame += 1\n",
    "        else:\n",
    "            max_flag = 1\n",
    "    except TypeError:  # error if hitting the end of the frame list\n",
    "        max_flag = 1\n",
    "\n",
    "expanded_scene_frame_pair = [scene_start_frame, scene_end_frame]\n",
    "\n",
    "expanded_scene_frame_pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we perform the above expansion for all the potential scenes we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_search = 6\n",
    "\n",
    "expanded_scene_frame_pairs = []\n",
    "for alternating_frame_pair in alternating_scene_frame_pairs:\n",
    "    expanded_scene_frame_pairs.append(expand_scene(alternating_frame_pair, vision_df, anchor_search_threshold=anchor_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[483, 527],\n",
       " [1786, 1806],\n",
       " [1822, 1842],\n",
       " [2909, 2950],\n",
       " [3187, 3227],\n",
       " [3293, 3339],\n",
       " [4369, 4458],\n",
       " [4415, 4525],\n",
       " [4973, 5050],\n",
       " [5064, 5124],\n",
       " [5095, 5198]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_scene_frame_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Check and Generating Scene Dictionaries\n",
    "\n",
    "Now that we have a list of *potential* scenes' beginning- and end-frames, there's one final check to make sure it's a two-character dialogue scene. We check to make sure there are indeed faces in the left-anchor and right-anchor shots.\n",
    "\n",
    "Although we assume the same character is in all the left-anchor shots, the face clustering sometimes may assign different face cluster values for different frames in a left-anchor shot (or right-anchor shot). We take the most prevalent face cluster value, and designate it as the `left_anchor_face_cluster` (or `right_anchor_face_cluster`). We also store the other face cluster values as `matching_left_face_clusters` and `matching_right_face_clusters`. We also store information on the cutaway shots.\n",
    "\n",
    "We can create a dictionary holding information for each scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1\n",
    "scene_dictionary_list = []\n",
    "for expanded_frame_pair, scene_anchor_pair in zip(expanded_scene_frame_pairs, alternating_scene_anchor_pairs):\n",
    "    first_frame = expanded_frame_pair[0]\n",
    "    last_frame = expanded_frame_pair[1]\n",
    "    scene_duration = last_frame + 1 - first_frame\n",
    "    expanded_face_df = face_df.copy()[first_frame - 1:last_frame]\n",
    "    expanded_vision_df = vision_df.copy()[first_frame - 1:last_frame]\n",
    "    left_anchor_shot_cluster = expanded_vision_df[(expanded_face_df['p_center_alignment'] == 'left') & (expanded_vision_df.shot_cluster.isin(scene_anchor_pair))].shot_cluster.value_counts().index[0]\n",
    "    left_anchor_face_cluster, matching_left_face_clusters = left_face_clusters(expanded_face_df)\n",
    "    right_anchor_face_cluster, matching_right_face_clusters = right_face_clusters(expanded_face_df)\n",
    "    right_anchor_shot_cluster = expanded_vision_df[(expanded_face_df['p_center_alignment'] == 'right') & (expanded_vision_df.shot_cluster.isin(scene_anchor_pair))].shot_cluster.value_counts().index[0]\n",
    "    cutaway_shot_clusters = vision_df[first_frame - 1:last_frame].shot_cluster.unique()\n",
    "    cutaway_shot_clusters = cutaway_shot_clusters[cutaway_shot_clusters != left_anchor_shot_cluster]\n",
    "    cutaway_shot_clusters = list(cutaway_shot_clusters[cutaway_shot_clusters != right_anchor_shot_cluster])\n",
    "    if left_anchor_face_cluster and right_anchor_face_cluster:\n",
    "        scene_dict = {'scene_id': x,\n",
    "                      'first_frame': first_frame,\n",
    "                      'last_frame': last_frame,\n",
    "                      'scene_duration': scene_duration,\n",
    "                      'left_anchor_shot_cluster': left_anchor_shot_cluster,\n",
    "                      'left_anchor_face_cluster': left_anchor_face_cluster,\n",
    "                      'matching_left_face_clusters': matching_left_face_clusters,\n",
    "                      'right_anchor_shot_cluster': right_anchor_shot_cluster,\n",
    "                      'right_anchor_face_cluster': right_anchor_face_cluster,\n",
    "                      'matching_right_face_clusters': matching_right_face_clusters,\n",
    "                      'cutaway_shot_clusters': cutaway_shot_clusters}\n",
    "        scene_dictionary_list.append(scene_dict)\n",
    "        x += 1\n",
    "\n",
    "    scene_dictionaries = {}\n",
    "    x = 1\n",
    "    for scene_dict in scene_dictionary_list:\n",
    "        scene_dictionaries[x] = scene_dict\n",
    "        x += 1\n",
    "        \n",
    "len(scene_dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are self-explanatory, but here are some clarifications:\n",
    "- left_anchor_shot_cluster: the shot cluster value of the left anchor\n",
    "- left_anchor_face_cluster: the most prevalent face cluster value of the left anchor shot\n",
    "- matching_left_face_clusters: all other face cluster values of the left anchor shot\n",
    "- cutaway_shot_clusters: shot cluster values of all shots that aren't anchor shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene_id': 1,\n",
       " 'first_frame': 483,\n",
       " 'last_frame': 527,\n",
       " 'scene_duration': 45,\n",
       " 'left_anchor_shot_cluster': 179,\n",
       " 'left_anchor_face_cluster': 3.0,\n",
       " 'matching_left_face_clusters': [11.0],\n",
       " 'right_anchor_shot_cluster': 86,\n",
       " 'right_anchor_face_cluster': 15.0,\n",
       " 'matching_right_face_clusters': [5.0],\n",
       " 'cutaway_shot_clusters': [39]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_dictionaries[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
