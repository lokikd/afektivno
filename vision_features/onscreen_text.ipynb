{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# onscreen_text\n",
    "This notebook uses the `PyTesseract`, or Python-tesseract library to detect on-screen text. This library is a wrapper for TesseractOCR, an optical character recognition engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import os\n",
    "import cv2\n",
    "from vision_features_io import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = load_frame('hustle', 285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This frame is part of an animated opening credit sequence with clear, legible text. The text is rendered onscreen, as opposed to actual footage (like a close-up of the words on a book's cover, or a store sign as characters walk past). \n",
    "\n",
    "We can use `image_to_string()` to automatically detect any text in the image. We'll be able to add this to our DataFrame: a Y/N flag if text was detected, and the actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text detected\n"
     ]
    }
   ],
   "source": [
    "if len(text) != 0:\n",
    "    print('text detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRO GOLDWYN MAYER\n",
      "PICTURES\n",
      "TS\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three lines of stacked all-caps text, and we managed to get the first two lines correct. However, the third line is in a much smaller font size, and we can only read the final two letters of the word \"Presents\".\n",
    "\n",
    "This implies we'll have trouble reading small text, and this may be a limitation of using 480p video files. In the future, frames from 720p or 1080p video files would help us capture text more accurately.\n",
    "\n",
    "We also weren't able to read text from a typical credits crawl. If we could, we could look for specific words (\"Producer\", \"Grip\", \"Production Assistant\", etc.) and determine the \"true\" runtime of the movie (by subtracting the duration of the credits)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
