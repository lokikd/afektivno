{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character_identification\n",
    "We can learn the names of characters by getting creative with the subtitle track. Character names are important for NLP-based plot comprehension. We'll also want to identify names and tie them to face and vocal encodings, to persistently track characters throughout the film.\n",
    "\n",
    "The audience learns who the names of characters by listening to the dialogue (except for the cases where character names are displayed onscreen, most often in documentaries or docu-dramatizations). So screenwriters know they have to put character names in dialogue. These might take the form of self-introductions \"I am Detective Lieutenant Elliot\" or more subtle hints like a line that addresses them in second-person \"I'm sorry, Marta.\"\n",
    "\n",
    "Screenwriters need to drop these hints when the character is introduced. But since we can analyze movies non-chronologically (all at once, in an instant), we can look for these types of clues everywhere.\n",
    "\n",
    "In previous notebooks, we've demonstrated how to parse and clean subtitles. For clarity, we'll just be typing in the lines of dialogue manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysrt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductions\n",
    "## Self-Introduction\n",
    "The most basic form of character introduction is the first-person introduction, which may take the form of \"I'm Alice\", or \"My name is Marlowe.\"\n",
    "\n",
    "This is a good time to clarify that many of the sentence structures we're looking for will be somewhat hard-coded. The two examples above are very common â€” there are only so many ways screenwriters can have a character introduce herself.\n",
    "\n",
    "### \"I'm Alan\" or \"I am Alan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Hey, I'm Vlad.\" # Teen Spirit (2018), subtitle 70\n",
    "sent_doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey hey INTJ UH intj Xxx\n",
      ", , PUNCT , punct ,\n",
      "I -PRON- PRON PRP nsubj X\n",
      "'m be AUX VBP ROOT 'x\n",
      "Vlad Vlad PROPN NNP attr Xxxx\n",
      ". . PUNCT . punct .\n"
     ]
    }
   ],
   "source": [
    "for token in sent_doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have spaCy analyze this simple sentence. This three-word sentence turns into six tokens. Vlad is properly labeled as a PROPN, a proper noun. Below, it also labels the words \"Detective\" and \"Lieutenant\" as proper nouns. This way we can get the character's full name, exactly how he introduced himself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I am Detective Lieutenant Elliot\" # Knives Out (2019), subtitle 64\n",
    "sent_doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X\n",
      "am be AUX VBP ROOT xx\n",
      "Detective Detective PROPN NNP compound Xxxxx\n",
      "Lieutenant Lieutenant PROPN NNP compound Xxxxx\n",
      "Elliot Elliot PROPN NNP attr Xxxxx\n"
     ]
    }
   ],
   "source": [
    "for token in sent_doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look for these two cases of self-introduction: \"I'm Alan\" or \"I am Alan\". We'll look for the tokens \"I\" and \"'m\" or \"I\" and \"am\", and then check if the next token is a proper noun. If it is, we'll look at the next token to see if it's also a proper noun, presumably another component of the character name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detective Lieutenant Elliot\n"
     ]
    }
   ],
   "source": [
    "pnoun_components = []\n",
    "pnoun_flag = 0\n",
    "start_token = 0 # this will be a parameter defining which token to start looking for \"I\", so we can iterate through\n",
    "\n",
    "if sent_doc[start_token].text == 'I' and (sent_doc[start_token + 1].text == \"'m\" or sent_doc[start_token + 1].text == \"am\") and sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "    while pnoun_flag == 0 and start_token + 2 < len(sent_doc):\n",
    "        if sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "            pnoun_components.append(sent_doc[start_token + 2].text)\n",
    "            start_token += 1\n",
    "        else:\n",
    "            pnoun_flag = 1\n",
    "\n",
    "pnoun_components\n",
    "string_value = ' '.join(pnoun_components)\n",
    "print(string_value)  # this will be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"My name is Alan\"\n",
    "We can do something similar for the phrasing \"My name is Alan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"My name is Henckels.\" # The Grand Budapest Hotel (2018), subtitle 642\n",
    "sent_doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henckels\n"
     ]
    }
   ],
   "source": [
    "pnoun_components = []\n",
    "pnoun_flag = 0\n",
    "start_token = 0\n",
    "\n",
    "if sent_doc[start_token].text in ['My', 'my'] and sent_doc[start_token + 1].text == \"name\" and sent_doc[start_token + 2].text == \"is\" and sent_doc[start_token + 3].pos_ == 'PROPN':\n",
    "    while pnoun_flag == 0 and start_token + 3 < len(sent_doc):\n",
    "        if sent_doc[start_token + 3].pos_ == 'PROPN':\n",
    "            pnoun_components.append(sent_doc[start_token + 3].text)\n",
    "            start_token += 1\n",
    "        else:\n",
    "            pnoun_flag = 1\n",
    "\n",
    "pnoun_components\n",
    "string_value = ' '.join(pnoun_components)\n",
    "print(string_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other-Introduction\n",
    "We can also identify phrases where someone introduces another character. We use similar logic as above.\n",
    "\n",
    "### \"This is Alan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent =  \"and this is Trooper Wagner.\" # Knives Out (2019), subtitle 65\n",
    "sent_doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trooper Wagner\n"
     ]
    }
   ],
   "source": [
    "pnoun_components = []\n",
    "pnoun_flag = 0\n",
    "start_token = 1\n",
    "\n",
    "if sent_doc[start_token].text in ['This', 'this'] and sent_doc[start_token + 1].text == \"is\" and sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "    while pnoun_flag == 0 and start_token + 2 < len(sent_doc):\n",
    "        if sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "            pnoun_components.append(sent_doc[start_token + 2].text)\n",
    "            start_token += 1\n",
    "        else:\n",
    "            pnoun_flag = 1\n",
    "\n",
    "string_value = ' '.join(pnoun_components)\n",
    "print(string_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interjection Cleanup\n",
    "A quick interruption, to deal with a common problem: there's an interjection, like an \"um\" or \"uh\". This will break our above logic. We can define interjection strings to be removed, and then redefine the sentence without it.\n",
    "\n",
    "These ums and uhs are pretty common in naturalistic comedies, so we'll be sure to remove these when we can. We may also choose to save these in the DataFrame, to denote that there was a hesitation/interjection in the original sentence, before we cleaned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"This is, uh, Maggie.\" # Plus One (2019), subtitle 619\n",
    "sent_doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Maggie.\n"
     ]
    }
   ],
   "source": [
    "interjection_string = ', uh,'\n",
    "\n",
    "found_sent = sent.find(interjection_string)\n",
    "if found_sent != -1:\n",
    "    sent = sent[:found_sent] + sent[(len(sent) - found_sent - len(interjection_string)) * -1:]\n",
    "\n",
    "sent_doc = nlp(sent)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look for an other-introduction in this cleaned sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maggie\n"
     ]
    }
   ],
   "source": [
    "pnoun_components = []\n",
    "pnoun_flag = 0\n",
    "start_token = 0\n",
    "\n",
    "if sent_doc[start_token].text in ['This', 'this'] and sent_doc[start_token + 1].text == \"is\" and sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "    while pnoun_flag == 0 and start_token + 2 < len(sent_doc):\n",
    "        if sent_doc[start_token + 2].pos_ == 'PROPN':\n",
    "            pnoun_components.append(sent_doc[start_token + 2].text)\n",
    "            start_token += 1\n",
    "        else:\n",
    "            pnoun_flag = 1\n",
    "\n",
    "string_value = ' '.join(pnoun_components)\n",
    "print(string_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
