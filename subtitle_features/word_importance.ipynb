{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word_importance\n",
    "To comprehend the plot of the entire film, or just an individual scene, we'll need to understand what the characters are speaking about. We'll use a variety of NLP tools to parse dialogue and try and see what the most important things they're talking about, be it other characters or concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subtitle_dataframes_io import *\n",
    "import pysrt\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a scene from *Plus One* (2019). We've previously defined a few functions to parse the subtitles file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pysrt.open('../subtitles/plus_one.srt')\n",
    "subtitle_df = generate_base_subtitle_df(subs)\n",
    "subtitle_df = generate_subtitle_features(subtitle_df)\n",
    "subtitle_df['cleaned_text'] = subtitle_df['concat_sep_text'].map(clean_line)\n",
    "sentences = partition_sentences(remove_blanks(subtitle_df['cleaned_text'].tolist()), nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of the film's entire sentences called `sentences`. We'll also define two more data objects at the scene level: `scene_sentences` which is a single long string of the scene's sentences, and then a `spaCy` doc of those scene-level sentences called `scene_nlp_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_sentences = (' ').join(sentences[880:976])\n",
    "scene_nlp_doc = nlp(scene_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Counting\n",
    "`spaCy` can conduct Entity Recognition, identifying people and organizations, as well as more abstract things like quantities or periods of time (like \"tomorrow\"). We can simply count up the most common entites in a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nate', 6),\n",
       " ('Alice', 3),\n",
       " ('Jess Ramsey', 2),\n",
       " ('first', 2),\n",
       " ('Ben King', 2),\n",
       " ('Maggie', 1),\n",
       " ('one night', 1),\n",
       " ('Alice Mori', 1),\n",
       " ('tomorrow', 1),\n",
       " ('two weeks', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "\n",
    "for ent in scene_nlp_doc.ents:\n",
    "    entities.append(ent.text)\n",
    "count = Counter(entities)\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scene, the pair (Alice and Ben King) is arguing about exes and prospective romantic partners: Nate, Maggie, and Jess Ramsey. This was a rough solution, but it worked, mostly because they're talking about named entities.\n",
    "\n",
    "This would be further improved if we could conduct pronoun resolution to link pronouns to their original subject. This string of sentences early in the scene is all about Maggie: *Um... she was good. Yeah she was... she was really... Yeah. She was cool.* If we could tell these sentences were about Maggie, she should appear higher in our list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Term-frequency - inverse document frequency is a measure of word importance that compares a word's number of appearances within a specific document, compared to its appearances within all documents. In other words, if a word appears often within a specific document, but not that often overall among all documents, it must be pretty important to that specific document.\n",
    "\n",
    "In our context, we're comparing the frequency of words in our scene against the frequency of words throughout the entire movie (minus this specific scene).\n",
    "\n",
    "So we'll have to build two data objects, `scene_doc` which contains the sentences of just the scene, and `film_doc` which contains all the film's sentences minus the scene's sentences. Then we'll create a two-element list for loading into the tf-idf object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_doc = sentences.copy()\n",
    "scene_doc = film_doc[880:976]\n",
    "del film_doc[880:976]\n",
    "scene_doc_joined = (' '.join(scene_doc))\n",
    "film_doc_joined = (' '.join(film_doc))\n",
    "film_scene_doc = [scene_doc_joined, film_doc_joined]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `scikit-Learn`'s TF-IDF capability. We can set the `ngram_range` argument to set the max tuple-size for phrases. In this case, we've set it to 3, meaning it may look for three-word phrases that work together like \"what the heck\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english', ngram_range = (1,3))\n",
    "idf_transformed = vectorizer.fit_transform(film_scene_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = pd.DataFrame(idf_transformed[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "tf_idf_df = tf_idf_df.sort_values('TF-IDF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.317364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>0.244126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>0.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nate</th>\n",
       "      <td>0.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>0.146476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.122063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>0.122063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>0.122063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moping nate</th>\n",
       "      <td>0.102933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moping</th>\n",
       "      <td>0.102933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TF-IDF\n",
       "know         0.317364\n",
       "just         0.244126\n",
       "don          0.170888\n",
       "nate         0.170888\n",
       "got          0.146476\n",
       "right        0.122063\n",
       "yeah         0.122063\n",
       "alice        0.122063\n",
       "moping nate  0.102933\n",
       "moping       0.102933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got us decent results, it identified that one of the characters is moping about Nate. Some of the phrase tuples may sound strange because stop words were removed â€” the phrase in the dialogue was \"moping over Nate\". And the word \"don\" is actually \"don't\", but this doesn't get picked up because the word was tokenized into two separate tokens \"don\" and \"'t\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene-Level Word Importance\n",
    "Now that we've been able to identify specific scenes, we can apply some of these techniques to encapsulated conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data_serialization')\n",
    "from serialization_preprocessing_io import *\n",
    "from time_reference_io import *\n",
    "from scene_identification_io import *\n",
    "from scene_details_io import *\n",
    "from character_identification_io import *\n",
    "from character_details_io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'plus_one_2019'\n",
    "srt_df, subtitle_df, sentence_df, vision_df, face_df = read_pickle(film)\n",
    "scene_dictionaries = generate_scenes(vision_df, face_df, substantial_minimum=4, anchor_search=8)\n",
    "character_dictionaries = generate_characters(scene_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict = scene_dictionaries[7]\n",
    "scene_start_time = frame_to_time(scene_dict['first_frame'])\n",
    "scene_end_time = frame_to_time(scene_dict['last_frame'] + 1) # add 1 second; scene ends one second after this frame is onscreen\n",
    "scene_subtitle_df = subtitle_df[(subtitle_df['end_time'] > scene_start_time) & (subtitle_df['start_time'] < scene_end_time)].copy()\n",
    "scene_sentence_indices = []\n",
    "x = 0\n",
    "for sub_index_list in sentence_df.subtitle_indices.values:\n",
    "    for sub_index in sub_index_list:\n",
    "        if sub_index in scene_subtitle_df.index.values:\n",
    "            scene_sentence_indices.append(x)\n",
    "    x += 1\n",
    "scene_sentence_df = sentence_df[scene_sentence_indices[0]: scene_sentence_indices[-1] + 1]\n",
    "scene_sentences = scene_sentence_df.sentence.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_sentence_doc = nlp((' '.join(scene_sentence_df.sentence.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look through all the scene's conversation's tokens, and identify people. From there, we can differentiate between a name being addressed and a name being discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ben']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_addressed = []\n",
    "for token in scene_sentence_doc:\n",
    "    if token.ent_type_ == 'PERSON' and token.dep_ == 'npadvmod':\n",
    "        people_addressed.append(token.text)\n",
    "people_addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lily']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_discussed = []\n",
    "for token in scene_sentence_doc:\n",
    "    if token.ent_type_ == 'PERSON' and token.dep_ != 'npadvmod':\n",
    "        people_discussed.append(token.text)\n",
    "people_discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ben, my parents don't like each other.\n",
      "I was bugged out at first when Lily asked me to be her maid of honor.\n"
     ]
    }
   ],
   "source": [
    "print(scene_sentences[20])\n",
    "print(scene_sentences[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
