{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subtitle_auxiliary\n",
    "We can define a few other subtitle functions to help us interpret films, which aren't related to the NLP-style analyses defined in other notebooks. These functions help with other interprations, such as determining the film's characters, or identifying scene boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subtitle_dataframes_io import *\n",
    "from collections import Counter\n",
    "from datetime import datetime, date, timedelta\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pysrt.open('../subtitles/plus_one.srt')\n",
    "subtitle_df = generate_base_subtitle_df(subs)\n",
    "subtitle_df = generate_subtitle_features(subtitle_df)\n",
    "subtitle_df['cleaned_text'] = subtitle_df['concat_sep_text'].map(clean_line)\n",
    "sentences = partition_sentences(remove_blanks(subtitle_df['cleaned_text'].tolist()), nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Identification\n",
    "### Dialogue Mentions\n",
    "By passing in a list of (cleaned) text consisting solely of dialogue, `spaCy` can count the number of times a name was spoken. This is a rough way of determining of the film's characters, the main characters are usually mentioned the most. We also define a blacklist of names we know aren't character names, but epithets like \"Jesus\" and \"God\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(' '.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ben', 95),\n",
       " ('Alice', 33),\n",
       " ('Dad', 21),\n",
       " ('Gina', 11),\n",
       " ('Brett', 7),\n",
       " ('Nick', 6),\n",
       " ('Matt', 5),\n",
       " ('Jess Ramsey', 5),\n",
       " ('Amanda', 4),\n",
       " ('Ben King', 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_blacklist = ['Jesus', 'Jesus Christ', 'Whoo', 'God', 'Mm', 'Dude', 'Mm-hmm', 'Huh']\n",
    "\n",
    "people = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.text not in people_blacklist:\n",
    "        people.append(ent.text)\n",
    "count = Counter(people)\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offscreen Speaker Names\n",
    "In many subtitle formats, especially SDH (subtitles for the deaf and hard-of-hearing), any character speaking from offscreen has their name in the subtitles for clarity, like *Anna: Open the door!*. We can count up the number of times this happens; like before, the main characters bubble up to the top. Again, we define a small blacklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ALICE', 51),\n",
       " ('BEN', 47),\n",
       " ('CHUCK', 14),\n",
       " ('ANGELA', 5),\n",
       " ('NICK', 3),\n",
       " ('MATT', 3),\n",
       " ('DAVIS', 3),\n",
       " ('DEEJAY', 2),\n",
       " ('BRETT', 2),\n",
       " ('ELLIE', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_counts = subtitle_df.speaker.value_counts()\n",
    "speakers = []\n",
    "speaker_blacklist = ['MAN', 'WOMAN', 'BOY', 'GIRL', 'BOTH', 'ALL']\n",
    "\n",
    "x = 0\n",
    "while x < len(speaker_counts):\n",
    "    if speaker_counts.index[x] not in speaker_blacklist:\n",
    "        speakers.append((speaker_counts.index[x], speaker_counts[x]))\n",
    "    x+=1\n",
    "speakers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Boundary Detection\n",
    "### Break in Dialogue\n",
    "We can use the subtitles to assist in determining where scenes begin and end. There's usually some \"breathing room\" after a scene ends, and the next one starts. We can look at the cleaned, dialogue-only text, and detect any 10-second span where there isn't any spoken dialogue (or laughter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_df index, subtitle_df index, start_time, delay length\n",
      "345 302 00:11:50.044000 0:00:33.826000\n",
      "365 319 00:12:43.264000 0:00:36.245000\n",
      "383 337 00:13:31.770000 0:00:12.429000\n",
      "389 343 00:14:04.678000 0:00:24.983000\n",
      "485 429 00:17:15.160000 0:00:11.970000\n",
      "664 588 00:23:37.918000 0:00:13.097000\n",
      "792 700 00:27:40.160000 0:00:23.524000\n",
      "817 722 00:28:43.473000 0:00:19.895000\n",
      "905 797 00:31:44.362000 0:00:22.398000\n",
      "1035 901 00:35:39.555000 0:00:27.361000\n",
      "1141 985 00:39:01.298000 0:00:20.604000\n",
      "1183 1020 00:40:46.695000 0:00:41.500000\n",
      "1351 1164 00:46:52.644000 0:00:27.611000\n",
      "1398 1198 00:48:27.823000 0:00:32.408000\n",
      "1427 1224 00:49:51.323000 0:00:12.096000\n",
      "1438 1232 00:50:28.777000 0:00:20.813000\n",
      "1695 1434 00:57:55.723000 0:00:12.137000\n",
      "1719 1454 00:59:08.004000 0:00:57.267000\n",
      "2131 1797 01:11:47.805000 0:00:10.428000\n",
      "2163 1825 01:12:55.163000 0:00:11.136000\n",
      "2254 1905 01:16:26.124000 0:00:14.055000\n",
      "2376 2018 01:21:25.006000 0:00:22.397000\n",
      "2414 2052 01:23:02.312000 0:00:15.266000\n",
      "2494 2128 01:26:55.837000 0:00:16.433000\n",
      "2666 2276 01:34:00.636000 0:00:12.095000\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "delay_threshold = timedelta(seconds=10)\n",
    "\n",
    "print('sentence_df index,', 'subtitle_df index,', 'start_time,', 'delay length')\n",
    "\n",
    "while x < len(subtitle_df):\n",
    "    if subtitle_df.iloc[x].cleaned_text or subtitle_df.iloc[x].laugh == 1:\n",
    "        y = 1\n",
    "        \n",
    "        while not subtitle_df.iloc[x - y].cleaned_text and subtitle_df.iloc[x - y].laugh == 0:\n",
    "            y += 1\n",
    "        delay = datetime.combine(date.today(), subtitle_df.iloc[x].start_time) - datetime.combine(date.today(), subtitle_df.iloc[x - y].end_time)\n",
    "        \n",
    "        if delay > delay_threshold:\n",
    "            print(x, subtitle_df.iloc[x].srt_index, subtitle_df.iloc[x].start_time, delay)\n",
    "    x += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
